\documentclass[11pt]{article}  

\input{packages_only_arxiv.tex}
\input{theorems_std.tex}
\input{macros.tex}


\newcommand{\citep}[1]{\cite{#1}}
\newcommand{\weight}[0]{w} %in case I want to change this


%\addbibresource{bib.bib}

\begin{document}

\title{AI Alignment}

\author{AISFP 2017}
%add names

\date{\today}
\maketitle
\begin{abstract}
Notes from the AI summer fellows program 2017.
\end{abstract}

\tableofcontents

\section{Cartesian and naturalized agents}
%Introduction}

\begin{enumerate}
\item
Consider a robot thinking about a video game: what inputs would cause me to win?

It thinks about what happens if it puts in inputs A and B; which input would give good outputs? It does something like argmax. 

\begin{figure}
\ig{pics/robot1}{1}
\caption{Cartesian agent}
\end{figure}
\item
Now consider a different world. In this world the robot imagines itself in this world. The game involves robots thinking about things like robots.

\begin{figure}[h!]
\ig{pics/robot2}{1}
\caption{Naturalized agent}
\end{figure}
\end{enumerate}
In the first setting, the robot thinks: Here's one thing I can do, and what it does to the world. It has copies of the world inside it. 

In the second setting: You can't fit a whole copy of the world inside the robot's brain because the robot's brain is in the world. There are no ``functions.'' We don't have a function the robot knows or a well-defined function the robot can learn about. There's nothing that feels like a ``base'' function.

In the second world, there could be other things that reason like the robot.

The first setting is a \vocab{Cartesian agent}---``plug into'' the world. The second is a \vocab{naturalized agent}. 

Question: what are naturalized agents like?

(You can introduce some complications in the Cartesian case by requiring the robot to be smaller than the world.)

There could be a thing in the video game that is a copy of the robot: the robot affects its input-output channel but also something else in the video game.

\subsection{Discussion}

\begin{enumerate}
\item
Q: In practice don't fully simulate the world/other agents. What about a partial simulation? But some robot which did more can exploit you?

The mindset is: We're focusing on creating Euclidean geometry, not do the engineering.
%set up multiple different ontologies that explain this.
\item
Only thing that controls is input/output. Contents of agent's head and thought process. You can't split it into input/output channel.
\item
Imagine I just thought of the world as neurons firing.  But isn't that isomorphic to knowing about myself?  The thought itself is an action.

``You're optimizing the thing that is doing the optimizing.''
\item
The field of game theory has part of ``naturalized agency,'' making the first picture look more like the second. Other agents are off the same type as you, but you view them as perfect adversaries. 
\item
Why is this interesting from AI alignment perspective?

%Industry will figure this out and we should figure out the alignment.
%things from the joint
Objection: It's better to stick together already existing tools and approaches/bits of confusion (Nash equilibrium, find more things to collide it with) than create something new from scratch.
\item
The I/O (Cartesian) model makes it difficult to think about cases when agent is rewriting its own internals. it doesn't help when the agent starts to grow up.
\end{enumerate}
Here's an outline.
\begin{enumerate}
\item
Cartesian world: 
First we look at what we understand about the first picture. That took a lot of time but we understand it well now. This gives us something to compare to.

What parts of what we understand here get messed up in the second picture, and what keeps working?

Then we do math. 
\item
Fixed point: Has most useful tools to think about this. Think about how fixed point theory applies to the picture.
\item
Philosophy
\end{enumerate}

\section{Cartesian agency}

We'll talk about Solomonoff induction and AIXI. 

\subsection{Solomonoff induction}

Solomonoff induction is the theory of learning if you have unbounded computational power.

This was introduced in the original conference on AI in the 1950's. The form that we have today was perfected in the 1990's.

You can get everything by formalizing Occam's razor very hard. We want to believe the simplest theory consistent with all the data, i.e. the simplest computer program generating all the data so far.

We want a Bayesian prior probability that conforms to this.

\begin{df}
%Turing defined 
A \vocab{Turing machine} has three tapes.
\begin{enumerate}
\item
an output tape. Once it outputs a bit it can't rewrite it.
\item
working tape going infinitely in each direction. You can move in either direction, and rewrite as you want.
\item
input tape (program tape), can't go backwards.
\end{enumerate}

A \vocab{universal Turing machine} $U$ is such that for any other Turing machine $T$ in this form, 
there is a program, i.e., a finite string of bits which we can put as prefix to the input to the universal Turing machine $U$,\footnote{Make sure no program is a prefix of another program. The probability of a program is $2^{-l}$ where $l$ is the length of the encoding.} before the input to $T$, so that $U$ will give the same output.
% so that it with end at position $l$ and all the rest of the inputs will be treated as inputs to the machine $T$.
\end{df}

\begin{df}
The \vocab{universal semimeasure} is a distribution on infinite bitstrings. When we feed in random bits to the universal Turing machine, we get bits out. $M(p)$ is the probability of getting $p$ as prefix to the output.
%, get bits out that are somewhat random 
\end{df}
%stop asking for input

\begin{figure}[h!]
\ig{pics/solo1}{1}
\caption{Solomonoff induction}
\end{figure}

Halting issues make $M$ uncomputable, but also makes something interesting happen.

Given 0100111, what's the probability that the next bit is 0/1? They don't sum to 1 because there's a probability that the machine doesn't output anything further.

Solomonoff thought this was a problem. The Solomonoff normalization is as follows.
\begin{df}[Solomonoff measure]
Conditioned on a next bit being outputted, what is the probability that it is 0/1?
\end{df}
Note this is different from conditioning on strings that give infinite output.\footnote{For the Solomonoff measure there's the odd thing of ``jumping'' to a different model if a model stops giving output at a particular step.}

%It's the unique (?) machine 
The distribution has the following nice property: If $l$ is length of shortest program that gives everything so far, then the probability of the output is at least $2^{-l}$. This gives a lower bound on probability of specific observation.

%fuzzy between program and data

Doing science is about predicting sensory observations, trying to figure out what the input bits might be that emitted the particular output (scientific observations).

Bayes loss quantifies prediction error in sensory  observations. %Predicting some particular bit, output a probability $p'$. 
The loss is $-\lg (p')$ where $p'$ is the probability you assign to the event that happens (ex. bit that is output).
%deep dark Bayes hell

Any amount of Bayes loss is because the universal measure assigned probability $2^{-l}<1$ to the correct program.

When you do Bayesian update, you cut out all programs which don't give the correct next bit. You must be increasing the probability of the correct program by $\rc{p'}$. (Renormalize good programs to sum to 1.) I can only increase it so much before it's 1.

I.e., Bayes loss is at most $-\lg\prc{2^l}=l$.

This is nice but also dissatisfying: we assume computable environment. It would be nice to deal with stochastic worlds, randomness in lives. 
Maybe we'll never learn true equations of physics or use them to fully predict output.

Can Solomonoff induction predict good stochastic models of the world? Yes.

We can emulate stochastic Turing machines. Consider a Turing machine taking in input and using randomness to give output. Use the rest of the coin flips on input tape of the UTM as randomness. (You lose probability mass every time, but not more than you would lose representing a probabilistic hypothesis.)

%finite loss according to ML method you claim works well on the environment.
%I can write some program that does better.
%I can talk about some not deterministic computable...
%treat with my ML method of choice
%SI can learn to emulate if indeed best way to predict environment
%Can have infinite loss if infinite randomness

You get finite loss compared to whatever other machine learning algorithm that works in that environment. (Total loss can be infinite if there is infinite randomness.)

\subsection{AIXI}

%reinforcement learning agent
We define a universal (reinforcement learning) agent that uses the universal semimeasure or Solomonoff induction.

The universal Turing machine has a new tape that is the action tape. The agent gives the actions which the environment takes in.

Some number of bits forms a time step. $R$ is a function of these bits and outputs a reward.

\begin{figure}
\ig{pics/aixi1}{1}
\caption{AIXI}
\end{figure}

Have a finite horizon which you treat as the end of time. Make a tree of possible actions %from
back to the current time. Maximize reward.

We can do things to get rid of the horizon function: use discounted reward and take the limit forwards.
%how can we make this ``consistent'' - and not e.g. fail at some future point?
%or keep pushing forward
%cognitive biases
Exponential discounting is temporally consistent.
%only way to have time-consistent preferences
%ex. money
%won't modify inconsistently because of discounting function 

In what sense is this optimal?
You don't necessarily get to the optimal policy. 

It's doing best thing according to universal distribution. You can modify AIXI: do some exploration---ex. open a door to find out what happens. If you do exploration in the right way, you can converge to the right optimal policy. %The reason you converge
But if there may be traps in the environment---opening the door may lead to infinite hell. ``You're doing all the things you should be doing in hell.'' This is the sense that you can converge to the optimal policy.

%You need to explore to do well, go in the trap.
Do we want Bayes optimal, optimal thing that falls in traps,...? This is unclear.

%AIXI-js

%(It could have a fear that looking out the window has a terrible problem.)

%Explore to best of ability.
%best policy

%VOI - value of information.
%finite regret?

\section{Advice}

\begin{enumerate}
\item
Have high (self-)standards for your understanding.
\begin{itemize}
\item
Original, not novel: ``novel'' means first time anyone has the idea. ``Original'' means you can regenerate them, explain them, translate them.
\item
Counter-arguments: Run the counterargument train on yourself over and over again.
\end{itemize}
\item
Nimbleness  (Notice when you're going down same paths and do something else)
\begin{itemize}
\item
Boggle
\item
Pre/post-formal distinction: 
\begin{enumerate}
\item
Pre-formal means you don't do any proofs. Reason using intuition and algorithms but can't give rigorous statements of theorem. 
\item
Formal: Crystal concepts, concepts and definitions that slot into each other, clear semantics, write formal proofs. 
\item 
Post-formal: interact with underlying math reality but still reason in intuitive way, understand the stuff behind the symbols. 
\end{enumerate}•
\item ``Raw thoughts'' as epistemic status tag.
\end{itemize}
\item
Attend to the central mysteries, like naturalization.
Even as you're doing math, think in the back of your head of agents reasoning; match up math to something that might fit into an agent.
\item
Courage.
\end{enumerate}

%3,4,6 same

\section{Diagonalization}

We have a reflection property and fixed point property. In many cases there is a simple relationship.

\begin{df}
The \vocab{reflection} property: $g$ is a surjection of a set onto a set of functions whose domain is the same set.
\begin{align}
g:S\tra T^S
\end{align}
\end{df}
\begin{df}
The \vocab{fixed point} property: For 
\begin{align}
f:T\to T,
\end{align}
$x\in T$ with $f(x)=x$.
\end{df}
Roughly, if you have the reflection property than every function has a fixed point.

Sometimes we use this in the positive direction; sometimes we use the contrapositive.

Fixed point sounds more mathematical, and reflection sounds more like agent foundations.

$T$ is like states of robot. For each state, do something about that.

Think of $T^S$ as a set of functions from $S$ to $T$ (ex. computable functions, continuous functions, etc.). 

\subsection{Examples}

We have a general proof that generalizes the proof that $\neg \exists \N\tra P(\N) = 2^{\N}$. 

\begin{figure}
\ig{pics/diag1}{1}
\caption{Diagonalization for $\N\tra P(\N)=2^\N$
}
\end{figure}
To show that statement, consider
$$
h: x\mapsto f(g(x)(x))
$$
where $f(0)=1$ and $f(1)=0$. Suppose by way of contradiction there is $n\in \N$ such that $n\in \N$, $g(n)=h$. Then
\begin{align}
g(n)(n)= h(n) = f(g(n)(n)),
\end{align}
so $g(n)(n)$ is a fixed point, contradicting definition of $f$. 
The proof is even in the positive direction, it (constructively) gives a fixed point. (Picking the $n$ is less constructive.)

We can also look at topological examples. If we have a map $[0,1]\tra \bS^{[0,1]}$ where $\bS$ is the circle (just the boundary).

The sides are the interval. Each ``row''  is a line of directions. Can we have a map from the square to functions $\bS^{[0,1]}$ that lists out all possible maps? No, we can look at the diagonal. This is an ``abstract'' diagonal.

\begin{figure}
\ig{pics/diag2}{1}
\caption{Diagonalization for $[0,1]\tra \bS^{[0,1]}$}
\end{figure}

%if not continuous, not necessarily continuous

%that is like the solution to 3
%constructively give a fixed point.

Currying relates functions of multiple arguments and functions of 1 argument valued in function space. $\wt g(x)(y)$ is the curried version of $g(x,y)$. $g:\N \to P(\N)=2^\N$ corresponds to $\wt g:\N\times \N \to \{0,1\}$.

%\begin{thm}
More generally, let $f:T\to T$ and $g:S\to T^S$. For all $x$, consider the function $S\to T$ defined by
$$
s\mapsto f(g(s,s)).
$$
The fixed point comes from the element of the set that codes for the function. If there is no fixed point, then the map is not surjective.

Lawvere's fixed point theorem works for all cartesian closed categories (roughly, there is notion of composition of functions and products). %\footnote{You can write things that look like programs (cf. lambda calculus).} 
%all the problems are a corollary.
All the problems have the same kind of proof, and are corollaries of Lawvere's fixed point theorem.
%\end{thm}
%surjectivity assumption

%not entirely constructive  but mostly
%constructively have ``do something different'', apply $f$. 

Quines use the same idea: call function with code of function as argument. There's a version you can do in computability-land, fixed point is not halting. Modify the construction to not have that problem; this tells you how to make quines.

%write instructions

\subsection{Lambda calculus}
There are also positive uses of Lawvere's fixed point theorem.

Lambda calculus is a minimalistic programming language. %haskell without actual haskell functions or datatypes
Everything is a function, functions are notated literally as lambda terms. For example $(\la \,x.\,x)$ is the identity function, and
$$
(\la \,x.\,x)(\la \,x\,y.\,y\,x) = (\la \,x\,y.\,y\,x).
$$
We can add more datatypes to give better examples. Let's say we also have arithmetic.
Then
\begin{align}
(\la \,f\,x.\,f\,(f\,x))(\la\,y.\,y+3)(5) 
&= (\la\,y.\,y+3)((\la\,y.\,y+3)(5))\\
& =(\la\,y.\,y+3)(8)=11\\
(\la\,x.\la\,y.\la\,z.\,z\,x) \,7\, (\la \,a. \,a+a) \,(\la\,b.\, b- 5) &=
(\la\,y.\la\,z.\,z\,7) \, (\la \,a. \,a+a) \,(\la\,b.\, b- 5) \\
&=
(\la\,z.\,z\,7)  \,(\la\,b.\, b- 5)\\
&=(\la\,b.\, b- 5)\, 7=2\\
(\la\,x.\la\,y.\la\,z.\,y\,(z\,x)) \,(\la\,a.\,2\cdot a)\,(\la\,b.\,b+3)\,10 &=23. 
\end{align}

Given $f:\La \to \La$, I want a lambda term $x$ that evaluates to $f\,x$:
$$f\equiv f \,x .$$
This is asking for a fixed point property.

The functions $\La\to \La$ are exactly the lambda terms, 
$$\La = \La^\La$$
so $f\in \La$. (This can be made precise by domain theory.)

Here we have a bijection $\La \tra \La^\La$, lambda terms just are the right sort of function. $g$ is implicit.
Construct $x$ by the diagonal construction. 

The diagonal map is $y\mapsto y\,y$ which corresponds to $(\la \,y.\,y\,y)$, and the function is $y\mapsto f \, (\la \,y.\,y\,y)$ which corresponds to $\la \,y. f \, (\la \,y.\,y\,y)$. We get
\begin{align}
(\la \,y. f \, (\la \,y.\,y\,y))\,(\la \,y. f \, (\la \,y.\,y\,y))
&= f\, \pa{
(\la \,y. f \, (\la \,y.\,y\,y))\,(\la \,y. f \, (\la \,y.\,y\,y))
}
\end{align}
%Use this trick to have $x$ be applied to
This is the Y-combinator. You can use it to implement recursion.
\begin{align}
Y&=\la \,f .\,(\la \,y.\,f(y\,y))(\la \, y.\, f\, (y\, y)).
\end{align}
(Note: 2 lambda terms are the same if they can be reduced to the same thing. However, this equivalence is not computable.)

\subsection{Applications to logic}
%{L\"ob's theorem}

I'll discuss applications to logic and to Agent Foundations.

I'll do logic with as little logic as possible. Let $L$ be a language like the language of Peano arithmetic. Let $L_n$ be formulas with $n$ free variables.

These are sentences in $L_0$:
\begin{align}
2+2&=3\\
\forall x.\, \exists y.\, y&>x.
\end{align}
These are formulas in $L_1$:
\begin{align}
2+n&=3\\
\forall x.\, x+n&=x.
\end{align}
Here $x,y$ are quantified variables and $n$ is a free variable.

$2+2=3$ is a sentence and $\ce{2+2=3}$ is code for a sentence (a number).

%We have a common application to , the diagonal lemma. 
One application is the diagonal lemma.
\begin{lem}[Diagonal lemma]
For all $\ph\in L_1$, there is $\psi\in L_0$ such that 
$$
\vdash\ph(\ce{\psi}) \lra \ph.
$$
\end{lem}
``$\vdash$'' means provable, $\vdash 2+2=4$, $\not\vdash 2+2=3$.

%operate as if operate on strings, ex. can easily OR.

Tarski's undefinability theorem: is there a formula $T\in L_1$ that codes for a true sentence? Suppose there is
$$
T(\ce{\psi}) \lra \psi.
$$
By the Diagonal Lemma, since $\neg T(n)\in L_1$, we can find $\psi$ such that 
$$
\vdash \psi \lra \neg T(\ce{\psi}).
$$
%provability vs. truth/definability/expressibility
%we know there are some undefinable predicates because countably many definable
%come predicates undefinable

\subsubsection{L\"ob's theorem}

A second application of the diagonal lemma is L\"ob's Theorem. Given $\psi$, let $\square \ce{\psi}$ be the statement that $\psi$ is provable. This is definable. (There exists a number encoding a proof of it.)

Under which conditions is 
$$
\vdash \square \ce{\psi} \to \psi?
$$
%self-fulfilling proofs

The box is a formula in our language (that takes in number as input), and the turnstile is meta-mathematically saying that what's to the right can be proved. 
%if sound, anything provable is true

%G\''odel's incompleteness theorem says you can't have consistency and completeness at the same time.

This is true when $\psi$ is provable. Also for self-fulfilling proofs. (If agent can prove it turns left, it turns left.) %If whenever $\psi$ is provable. 
Are there other sentences for which this is provable? L\"ob's Theorem says no.

\begin{thm}[L\"ob's Theorem]
$$
\vdash \square \ce{\psi} \to \psi \implies \vdash \psi.
$$
\end{thm}

Suppose an agent is such that if the agent finds a proof that it goes left, it goes left. Then L\"ob's Theorem says it goes left.

L\"ob's sentence is constructed using the diagonal lemma.

In the context of an agent, this has weird consequences. We might want it to believe soundness about itself: believe things it's proven, i.e., assert anything it's proven is true. If it assigns probabilities, consider the sentences it assigns probability 1 to. It can self-referentially form the sentence
\begin{align}
\Pj(\phi)=1\to \phi.
\end{align}
This is soundness applied to belief rather than provability.

L\"ob's Theorem only uses simple facts about provability such as 
$$
\vdash \square \ce{\psi} \to \square \ce{\square \ce{\psi}}.
$$

In the belief case, you can recover many properties of provability. If agent assigns probability 1 to $\Pj(\phi)=1\to \phi$, it has to already believe (assign probability 1) to $\phi$. 
%limit way think about itself in way that it's surprising.

We can think about the agent as wanting to plan for its future self. It's a desirable property that the agent at time 0 will think it will form correct judgments at time 1. Then the agent at time 1 has to be doing proofs in a different theory than at time 0. If agent at time 1 is doing proofs in PA and so is the agent at time 0, then this property can't hold.
$$
\vdash_{ZFC} \square_{PA} \ce{\phi}\to \phi.
$$
Reduction in power is not a satisfying way of solving the problem.

There is a bounded variant of L\"ob's theorem (up to proofs of certain length).

%\forall\psi( \square \ce{\psi}\to \psi)\to \psi
%then every statement is provable, and not consistent.
%turnstil and box could refer to different systems. PA+soundness as turnstile.

%obfuscate trick from system itself
%unsound?

What if you're always in ZFC, trusting subcomponent of PA?

You can have partial self-modification as long as you can prove to trusted core that modification is OK. But you can't convince the trusted core to modify itself.

Coherence and reflection conditions on probability distribution.
\begin{df}
$\Pj$ is \vocab{coherent} if
\begin{enumerate}
\item
If $\vdash \perp \ph'$ then $\Pj(\ph)=1$. 
\item
If $\vdash \ph\to \psi$ then $\Pj(\ph) \le \Pj(\psi)$. 
\item
$\Pj(\ph\wedge \psi)+ \Pj(\ph\vee \psi) =\Pj(\ph) + \Pj(\psi)$.
%$\Pj(\ph)<a $ iff $\Pj(\Pj(\ph)<a)=1$.
\end{enumerate}
$\Pj$ has \vocab{reflection} if
$$
\Pj(\ph)<a \text{ iff } \Pj(\Pj(\ph)<a)=1.
$$
\end{df}
%LI far from coh?
$\Pj$ can't be coherent and reflective at the same time.

\subsubsection{L\"obian handshake}
There is 1 positive thing we can do with L\"ob's Theorem that lets us implement something we would want to implement: L\"obian handshakes to implement coordination in certain game-theoretic situations such as Prisoner's dilemma.

If the robot proves that it will turn left, it will turn left (assuming it is provable in the same system).

Consider 2 agents $A, B$, playing Prisoner's Dilemma, each cooperates if it can prove the other agent cooperates.
\begin{align}
A&=\text{coop} \lra \square \ce{B=\text{coop}}\\
B&=\text{coop} \lra  \square \ce{A=\text{coop}}.
\end{align}
We have $A=\text{coop}\wedge B=\text{coop} $ because
\begin{align}
\vdash& \square 
\ce{A=\text{coop} \wedge B=\text{coop}} \to 
A=\text{coop}\wedge B=\text{coop}\\
\vdash &A=\text{coop}\wedge B= \text{coop}.
\end{align}
They are looking for the cooperation handshake. Decide what handshake to look for, define what looking for means...
\begin{enumerate}
\item
If you replace coop with defect, then both defect.
\item
If A coops only if it proves B coops, and B defects only if it proves A defects. Neither can prove what the other does, so A defects and B cooperates.
\end{enumerate}•

%what if other defect
%not mutual coop
%neither can prove. 
%defect, coop

\section{Game theory}

\subsection{Nash equilibria}

A prisoner's dilemma has the payoff matrix

\begin{center}
\begin{tabular}{c|c|c|}
& C & D\tabularnewline
\hline
C& 2,2 & 0,3\tabularnewline
\hline
D&3,0 &1,1\tabularnewline
\hline
\end{tabular}
\end{center}

No matter what the other player does, I'm better off defecting. It's a dominating strategy to defect. It's a pure Nash equilibrium for both players to defect.

\begin{df}
$(A,B)$ is a  \vocab{Nash equilibrium} if no player has an incentive to change their actions, 
\begin{align}
u_1(A,B)&\ge u_1(A',B) &\forall A'\\
u_2(A,B)&\ge u_2(A,B') &\forall B'.
\end{align}
\end{df}
There are games for which there doesn't exist a pure Nash equilibrium, for example, matching pennies.

Row player wins if they put pennies the same way and column player wins if they put pennies different ways.

\begin{center}
\begin{tabular}{c|c|c|}
& H & T\tabularnewline
\hline
H& 1,-1 & -1,1\tabularnewline
\hline
T&-1,1 &1,-1\tabularnewline
\hline
\end{tabular}
\end{center}

%This is already pictured with our Cartesian agency.

First we'll suppose the other player's action is fixed; how does my action control the utility?

\begin{enumerate}
\item
Suppose $P(p_2=H)<\rc 2$. Then I want to pick tails.
\item
Suppose $P(p_2=H)>\rc 2$. Then I want to pick heads.
\item
If $P(p_2=H)=\rc2$, I am indifferent. (E.g. I could randomize.)
\end{enumerate}•
The intuition is what you should do is randomized. We can formalize this with \vocab{mixed Nash equilibrium}. We have the same defining conditions but the strategies are probabilities.

There is a unique mixed Nash equilibrium for matching pennies. 
\begin{enumerate}
\item
Player 1 plays $\rc2 H + \rc2 T$.
\item
Player 2 plays $\rc2 H + \rc2 T$.
\end{enumerate}

Note this is intentioned with the non-naturalized, Cartesian picture, and it's not clear what the naturalized picture is.

%no incentive to randomize
%superrational FDT

%If second agent 
%predictable and exploitable
%In nash equilibrium, both players doing argmax over other player. 
The world cannot simultaneously pick the best thing in response to what you're doing without breaking.
We're thinking of this as a 2-player game, responding to what they expect other player to do. This is already bringing in some complexities (but not all) from naturalization. We still have a well-defined interaction with the world but there's a recursive thing that can go on with the agents.

The world can't optimize to take advantage of you but other agents can. This is a problem if we can't identify agents in the environment; the game theory versus players and the environment is different.
%break if put in agents, don't 
%if can't identify agents in env
%separate magisteria: game theory vs. players and environment.

Our outline is the following.
\begin{enumerate}
\item
Prove existence of Nash equilibria.
\item
Philosophy
\item
Reflective oracles
\end{enumerate}•
Logical induction gives better philosophical grounding than reflective oracles, but we'll discuss this later.

%It's hard to find because fixed point theorems aren't that constructive.

Kakutani's fixed point theorem is stated in terms of set-valued functions.
\begin{thm}[Kakutani's fixed point theorem]
Let $S$ be a nonempty compact convex subset of a Euclidean space.

Let $f:S\to 2^S$ be a set-valued function (multi-valued function) with a closed graph $\set{(s,t)}{t=f(s)}$ pointwise nonempty and convex. (Each $f(s)$ is a nonempty convex set.)
%can take multiple values, but not no values.
\end{thm}
For example, an infinitely steep S-curve is such a multivalued function.

\ig{pics/game1}{1}
We apply this to $S=S_1\times S_2$, the total strategy space. Here $S_i$ is the space of mixed strategies over actions available player $i$, $S_i = \De (A_i)$, the simplex of probability distributions over the actions.

For example, when there are 2 actions, $S=[0,1]^2$ is a square.

\begin{figure}
\ig{pics/game3}{1}
\caption{Functions which satisfy the conditions in the Kakutani fixed point theorem}
\end{figure}

Then there exists $s\in S$ such that $s\in f(s)$.

Something which is not the same as Kakutani functions but which is useful for intuition pumps is the limit of a sequence of continuous functions (e.g. become steeper and steeper). It also has a fixed point property.

Consider a $n$-player game, $S=S_1\times \cdots \times S_n$. Define the best response function $f:S\to \cP(S)$ by
\begin{align}
f(s_1,\ldots, s_n) &= 
\set{s_1}{\forall s_1'\in S_1,\quad u_1(s_1,\ldots, s_n)\ge u_1(s_1',\ldots, s_n)}\times \cdots \subeq \cP(S).
\end{align}
Kakutani's Theorem gives the existence of
$$
(s_1^*,\ldots, s_n^*)\in f(s_1^*,\ldots, s_n^*).$$ 
Then no one has an incentive to deviate so this is a Nash equilibrium.

For example, the game of chicken: we drive towards each other. A person who turns aside (swerve) is chicken and lose; if both people dare, they crash into each other.
S is swerve and D is dare.

\begin{center}
\begin{tabular}{c|c|c|}
& S & D\tabularnewline
\hline
S& 0,0 & 0,1\tabularnewline
\hline
D&1,0 &-10,-10\tabularnewline
\hline
\end{tabular}
\end{center}

When player 1 swerves, player 2 definitely wants to dare, when player 1 dares, player 1 definitely wants to swerve.

\begin{figure}
\ig{pics/game4}{1}
\caption{Game of chicken}
\end{figure}

The 2 functions are individually Kakutani functions, put them together to get $S_1\times S_2\to \cP(S_1\times S_2)$. 

There are 3 different Nash equilibria where the graphs intersect, 2 pure ones. In the mixed one, each dares with small probability.

\subsection{Naturalization}

%curious and interested about the things

We used Brouwer to do game theory (Brouwer and Kakutani are basically the same thing). Further applications:
\begin{enumerate}
\item
We use Brouwer to address coherence and reflection: you can't have coherent probabilities and reflection because of L\''ob's Theorem, one of the problems Lawvere caused. We used Brouwer to overcome it by assigning infinitesimal probabilities to things.

Tarski's Theorem: You can't write down what it means for something to be true because then you could diagonalize against yourself: $\neg T(\ce{\phi}) \lra \phi$. 
Instead of having truth values being true or false, we can have continuous truth values. If not is $1-\bullet$, then we can make the truth value $\rc2$... we can use Brouwer to have a consistent way of saying whether or not things are true. We get reflection within any infinitesimal bound, one direction but not the other.
\item
We can solve the halting problem by having an oracle that gives the answer. Once you've added, you want to ask when do machines with that oracle halt? You can continuously add and have a hierarchy. You can add an oracle to the robot, but it's reasoning about things with a oracle, which breaks it.

We want to give something like a halting oracle---reflective oracle---it doesn't just tell you whether normal machine halt, but whether machines with a reflective oracle halt. It does this probabilistically. You ask it questions of the form, does this halt with probability $>\rc2$. Answering randomly is like having a Kakutani-type thing. 
\item
The last thing we use Brouwer for is an agent reasoning about its future self, the L\"obstacle. Using logical induction we can model this pretty well; have things that trust themselves almost completely. They manage it without diagonalizing against themselves by using Brouwer.

\end{enumerate}•



A lot of problems we had before in Lawvere world we are good in Brouwer world. But in Brouwer we can't come up with a good way to choose the fixed point.

If I were to move my function around, I can bring it back to where it started, and if you move the fixed point continuously you would end up somewhere else.

There are these fixed point god things that choose where you go.

In game theory you have a whole bunch of Nash equilibrium.

It would be nice to have 1 rather than 2 fundamental truths about the universe (Lawvere, Brouwer). I'm curious to whether you can connect them.

We showed we can't have a surjective function $X\tra (X\to \bS)$ because there is a map $\bS\to \bS$ without fixed points.

Is there $X\tra (X\to [0,1])$? Then you can prove Brouwer using Lawvere.  I know what the proof would look like if I can find $X$...

Another reason I would like this: have model of agents reasoning about other agents. Agents react to other agents' policies.
\begin{enumerate}
\item
Imagine $X$ is a space of codes (a topological space) a program can have.
\item
$X\to [0,1]\to [0,1]^X$ is a space of policies. I look at opponent's code and cooperate with some probability.
\item
 $X\to (X\to [0,1])$ is interpreter that takes in code and returns policy. %; encode policy with come code. 
 Code implements policy $X\to [0,1]$ when you run it.
\end{enumerate}

This seems to connect to fundamental secrets of the universe.

\section{Decision theory}

I'll work in a space where there's not a distinction between CDT and EDT, or we're using EDT.

We have assignments of probabilities $P_n$ (or expectations $\E_n$) to logical sentences. We build decision theory out of this. 

Consider an agent $A_n$, choosing between action 1 or 0. 
\begin{align}
A_n &= \begin{cases}
1,&\text{if }\E_n(U_n | A_m=1)> \E_n(U_n|A_n=0)\\
0,&\text{otherwise.}
\end{cases}
\end{align}
For logical inductors, for many properties, in the limit, as we make the parameter large enough, the property holds. We would like for $A_n$ to choose the better action in the limit.

For this talk, assume all utilities are bounded.
Suppose $U_n=A_n$.

Given all the things logical inductors do well, you might think it might take action 1 in the limit. 

Logical inductor learns by feedback. If it takes action 1 all the time, it can never really learn what happens if it takes action 0. It could take action 1 all the time.

Alternatively, it could take action 0 all the time, believe if it takes action 0 it gets 0 reward, and believe if it takes action 1 it gets $-10^9$ reward.

This is very much like evidential decision theory. The problem is that if you take a conditional on something with probability 0, bad things happen.

You can get this phenomena in all kinds of things. When you condition on something approaching 0, things can be bad.

This is a problem for EDT. For CDT we give it a big structure: this is the node that's you, for each action this is what happens. I understand how to optimize that, using $\amax$. 

In our picture, if I didn't take action A, it's hard to say what happens if I did take action A.

(There may be agents that search for proofs. Isn't there a short proof that $A_n=1$ gives $U_n=1$? But there may also be a proof that $A_n=1$ gives $U_n=-10^9$.)

Note the problem can only happen all the way---if the agent chooses 1 half the time, then it will see what happens in both cases.
A (bad) way to deal with this is $\ep$-exploration. Do random thing with probability $\ep$, and learn what things happen.

\begin{align}\label{eq:dec2}
A_n &=
\begin{cases}
1&\text{if }\Pj_n (A_n=1)<\ep\\
0&\text{if }\Pj_n(A_n=0)<\ep\\
1&\text{if }\E_n(U_n|A_n=1)> \E_n(U_n|A_n=0)\\
0&\text{otherwise}
\end{cases}•
\end{align}
If you know you're not going to take the action, take the action. This is different from randomizing.
If the agent is almost certain won't take action, will take action just to see what happens. 

What happens is if it's almost certain it will take an action, it shifts the probability down to $1-\ep$.

This is like the liar's sentence (this sentence is true with probability $<\rc2$---randomize when probability is around $\rc2$). %if I assign it
%If I assign it probability $\rc2$ it 
The probability would converge to $\rc2$ and wiggle so it's above/below it half the time.

This algorithm is unsafe: When you take actions, you don't just lose the local game---What happens if you self-modify into a chicken?

Ignoring this, it works, because when it takes conditional probabilities, it takes them bounded away from 0.

Now consider 
\begin{align}\label{eq:dt-pd}
U_n = 10\Pj_n(A_n=0)+A_n.
\end{align} %There's 2 ways to think about this. One
This is %of 
playing a prisoner's dilemma against an agent of similar power that thinks ``I will coop with probability I think you cooperate.''

You want it to believe it's going to choose 0 and play 1. But you can't get both!
%learn from past examples

%not exactly past frequencies but close
%mixture of things trying to predict what you'll do based on what you've done.

I claim that the agent defined by~\eqref{eq:dec2} defects all except $\ep$ of the time.
Suppose it cooperates most of the time. 
The times it randomly defected, its opponent doesn't predict it, so it gets reward!

%opaque box, don't view structure of stuff
%if press other button, extra box
%I view as opaque 
When I explore and defect, it starts paying me less. At some point I get suspicious... But if you try cooperating you get even less again...

The logical inductor already knows $\Pj_n(A_n=0)$. So when it does expected value calculations, it can treat it as a constant.

%it goes into wrong fixed point?

Suppose you're playing Prisoner's Dilemma against someone with similar prediction skill to you and knows you about as well as you know yourself. They're going to predict whether or not you cooperate.

%Set up future me for failure.

Imagine you didn't know the fact, $U_n=10\Pj_{f(n)}(A_n=0)+A_n$ where $f(n)$ is a fast-growing function. Now pretend the predictor is really good. It's going to cooperate.

%Before I had a weak predictor. When I made the predictor strong.

%deterministic program lives in logic.
%it's not obvious this is the wrong thing to do.

%Q: what if lookforward?
%Q: why can't realize if you flip?

%why different based on how it is being predicted.

%what if someone predicts you as well as you predict yourself?

%They're equally good at predicting you as you are at predicting yourself.

%deterministic algorithm for each $n$

%for each $n$ it has a value 0 or 1
%built using quining and logical inductor
%you're being predicted using same LI
%in one case, using current beliefs, in other case, using future (more accurate) beliefs.


\subsection{Updatelessness}

The standard example: I flip a coin and it comes up heads. You trust me and I'm a good predictor of you. I decided I would give you \$100 if it comes up tails but only if I predicted you gave me \$10 if it come up heads.

Now I'm asking you to give me \$10. Do you give me \$10?

%Coin is H
%If you give 10 dollars on H, you get 100 dollars on T.

It's important that you actually give me \$10 in this situation. If yesterday you knew you wouldn't, you want to self-modify to give me \$10. Then you're not reflectively consistent: you want to modify yourself into something consistent.

I claim that the thing going on in~\eqref{eq:dt-pd} is similar.

The way we solve this is by going to the beginning of time. Rewind the tape, you didn't tell me this yet. In my prior I have some prior probability that the coin comes up heads. I work on that prior probability. I choose the policy (if it comes up heads I give \$10, if it comes up tails I get you \$100). 

I don't update on the coin flip. I keep original probabilites on coin flip. %; choose a policy that is a policy from observations to what I did.

Updatelessness: the coin came up heads but there's this other multiverse where the coin came up tails and I want to help that version.

Updatelessness is good but it's set up in a Bayesian framework.

A prior at the beginning of time is very big, if it has all the logical statements... We don't know how to combine the idea of updatelessness with computation. We can only state it in Bayes's world.

%Suppose the future version of you can see the past version.
%Logical induction is incompatible with having a prior on logic.

%simulations nonconscious
%I don't know I'm not

%proof that millionth digits of pi is 0, 1
%You have to be simultaneously looking at proof and optimizing for world where millionth digit is 0.
What if instead of ``coin is H'', it's ``millionth digit of $\pi$ is odd''? Suppose Omega is not adversarial (in choosing the millionth digit). Then if you don't do the same thing, you're not reflectively consistent!
%Omega is not adversarial

Logical updatelessness is not understood. 

%part of me in different multiverse
%make handshakes with each other
%probability world where we've solved this problem

\section{Honest induction problem}

This is also known as Goodhart's law, or the rocket problem.
%robust induction

\begin{enumerate}
\item
Rocket problem/Goodhart's law
\item
Logical induction as robustness solution.
\item
Examples/intuitions
\end{enumerate}•
We ask a robot to build rockets. The robot goes off and thinks about rockets. The robot comes up with plans for how a rocket should look. It returns a rocket plan to us.

\begin{figure}[h!]
\ig{pics/rocket}{1}
\caption{The rocket problem}
\end{figure}

We look over the plan and evidence that went into its design, and evaluate how good the rocket would be, for example, how good it would be at getting to the moon. 
There are many reasons this could be hard.

Let $E_{\text{me}}$, $E_{\text{robot}}$ be the evidence I and the robot have gathered.
We have posteriors
$$
\Pj(\cdot |E_{\text{me}}) \quad \Pj(\cdot |E_{\text{robot}}).
$$
 If we were both Bayesians, we can ask the robot to give all its evidence to me, and get 
 $$
 \Pj(\cdot |E_{\text{me}}\wedge E_{\text{robot}}).
 $$
I would be at an epistemic advantage compared to me or the robot before.
We can't do this with logical inductors.
 
The most important sense in which logical inductors aren't Bayesian is that we can't point to the evidence.

Can we look into a Solomonoff inductor or neural network and ask why does it believe what it believes?

\vocab{Goodhart's law} says:
When a measure is used as a target, it ceases to be a good measure. In normal cases it is correlated with some desired objective. When it is being optimized, it becomes an extreme case and it may stop being correlated with the objective.

A lot of scenarios of AI going wrong are like this, e.g. paperclip optimizer.

%talk about how good beliefs are
%missing something
%proxy that when pushed to extreme range stops being good measure.

We have good results about logical induction. How will a logical inductor actually do in practice?

Consider $n=10^{10}$. 
What traders have a lot of wealth on this day, and what are they predicting. Traders that did a good job early on are wealthy. They found ways the market or other traders were being stupid. A trader has a list of other statements and considers what it should bet on. It has a large amount of time to compute various facts that help: what to think about more, should I think about what other traders do, do I look for inconsistencies between different facts on the market... It has computational resources and has to think about how to allocate them.

Personifying the trader,  I think some simple relationships should hold; if they are outside bounds, make a trade on it. This doesn't mean these traders will earn a lot of money early on.
%runtime 
Traders have initial weight dependent on complexity of program describing trader. It seems this is relatively simple and uses little computation.

I expect things like this exist and get money early on; later on other traders stop throwing money away, the trader starts being more cautious, market becoming more rational over time... Specifying all these things separately seems more complicated than having general-purpose optimizer. 

If I ask my logical inductor about statements, I'm asking the trader, what will you tell me about this statement?

%honestly tell me if we impute beliefs to it
If it is optimized to do other things, it would try to control the external world, and the first thing it would do (for instrumental reasons) is to make good predictions.
%It may be instrumentally incentivized 
Making good predictions so far is often a proxy for making good predictions in the future. %When we push to extreme values, we have to think
But will it continue to make good predictions? 
%Has it made good predictions so far, and why should I expect these correlations to 
Why should I expect it to continue to make good predictions?
%continue to work?

%Are we searching a space of programs that are optimizing generally?

I want to update on what my robot believes about rockets. %My robot acts closely enough
%Logical inductor
Then we have this Bayesian intuition that I have a good idea what happens in the future. Since we don't have this Bayesian picture, we can't just update on evidence. 

Traders give output, and we have this Goodhart question of whether we can trust the output.

%wait until particular point in time and do something weird.

Solomonoff induction is a reasonable prior but has all computations inside of it. Any computation could have something to say about the world.

%economic reasons for goodhart's law.

%can't trust things robot decides to say.
%things we can trust in certain way by logical induction process, come up with robust predictions
%Suppose we build rocket. Look forward to predictions of launch
%trader sabotaged rocket.
%make bets rocket will succeed.
%stuff doing actual thinking is making ...
We want to impute traders as having beliefs. Logical inductor has beliefs, but the traders are just structured to be an algorithm.

%The amount that Solomonoff induction expects $2^n$ after Big Bang to be weird is just as much as I think
%what the natural category is
%complexity penalty

%trap

%We don't want to assume bounded loss at each step.

%nonconvergence of utility under SI?

%hyp specify situation
%also specify utility function and attach function
%complexity goes down
%util specified goes up as BB(n)

%1 trader doing treacherous turn.

%Is one LI strictly more informed than another? Ex. one has access to another's sensory data. One is no less trustworthy than another?

%trust them more if got rest of architecture right?

%one LI does well
%planck interval

\subsection{Naturalization as instance of honest induction problem}

If I had Solomonoff induction, sequence prediction predicting all the facts about the universe, then maybe the universe is simple, and we can trust the prediction.

That's not a situation that's physically possible to put a predictor into. Instead the predictor has e.g. cameras looking at particular things. I'm interested in photons coming from this tree. This is way more algorithmic information than the universe.

We ask: which hypothesis has short description of %universe and
 this camera feed?
%quantum branch of universe

%(We don't expect the simple explanation to have the rest of the universe.) Programs length $n$. some other programms $n-\lg(bajillion)+constant$ produces same output. Specifying outputs in certain order, arithmetic code
%which one is simpler hyp

%It's hard to read out from physics what the camera is doing.
General purpose optimizers making predictions of the world with an incentive can be specified in fewer bits than that. %incentivize to follow camera.
It's fewer bits to follow the camera (generating the same bits) by being a general-purpose optimizer than having a general theory.
%simple goal to want to follow camera

\ig{pics/universe}{1}

%agents in math space, care about universe

%In SI, have hypotheses that are computer programs printing out infinite sequences of bits.

%crawl down from math into the world


%\printbibliography
%\appendix


\end{document}

