\documentclass[11pt]{article}  

\input{packages_only_arxiv.tex}
\input{theorems_std.tex}
\input{macros.tex}


\newcommand{\citep}[1]{\cite{#1}}
\newcommand{\weight}[0]{w} %in case I want to change this


%\addbibresource{bib.bib}

\begin{document}

\title{AI Alignment}

\author{AISFP 2017}
%add names

\date{\today}
\maketitle
\begin{abstract}
Notes from the AI summer fellows program 2017.
\end{abstract}

\tableofcontents

\section{Cartesian and naturalized agents}
%Introduction}

\begin{enumerate}
\item
Consider a robot thinking about a video game: what inputs would cause me to win?

It thinks about what happens if it puts in inputs A and B; which input would give good outputs? It does something like argmax. 

\begin{figure}
\ig{pics/robot1}{1}
\caption{Cartesian agent}
\end{figure}
\item
Now consider a different world. In this world the robot imagines itself in this world. The game involves robots thinking about things like robots.

\begin{figure}
\ig{pics/robot2}{1}
\caption{Naturalized agent}
\end{figure}
\end{enumerate}
In the first setting, the robot thinks: Here's one thing I can do, and what it does to the world. It has copies of the world inside it. 

In the second setting: You can't fit a whole copy of the world inside the robot's brain because the robot's brain is in the world. There are no ``functions.'' We don't have a function the robot knows or a well-defined function the robot can learn about. There's nothing that feels like a ``base'' function.

In the second world, there could be other things that reason like the robot.

The first setting is a \vocab{Cartesian agent}---``plug into'' the world. The second is a \vocab{naturalized agent}. 

Question: what are naturalized agents like?

(You can introduce some complications in the Cartesian case by requiring the robot to be smaller than the world.)

There could be a thing in the video game that is a copy of the robot: the robot affects its input-output channel but also something else in the video game.

\subsection{Discussion}

\begin{enumerate}
\item
Q: In practice don't fully simulate the world/other agents. What about a partial simulation? But some robot which did more can exploit you?

The mindset is: We're focusing on creating Euclidean geometry, not do the engineering.
%set up multiple different ontologies that explain this.
\item
Only thing that controls is input/output. Contents of agent's head and thought process. You can't split it into input/output channel.
\item
Imagine I just thought of the world as neurons firing.  But isn't that isomorphic to knowing about myself?  The thought itself is an action.

``You're optimizing the thing that is doing the optimizing.''
\item
The field of game theory has part of ``naturalized agency,'' making the first picture look more like the second. Other agents are off the same type as you, but you view them as perfect adversaries. 
\item
Why is this interesting from AI alignment perspective?

%Industry will figure this out and we should figure out the alignment.
%things from the joint
Objection: It's better to stick together already existing tools and approaches/bits of confusion (Nash equilibrium, find more things to collide it with) than create something new from scratch.
\item
The I/O (Cartesian) model makes it difficult to think about cases when agent is rewriting its own internals. it doesn't help when the agent starts to grow up.
\end{enumerate}
Here's an outline.
\begin{enumerate}
\item
Cartesian world: 
First we look at what we understand about the first picture. That took a lot of time but we understand it well now. This gives us something to compare to.

What parts of what we understand here get messed up in the second picture, and what keeps working?

Then we do math. 
\item
Fixed point: Has most useful tools to think about this. Think about how fixed point theory applies to the picture.
\item
Philosophy
\end{enumerate}

\section{Cartesian agency}

We'll talk about Solomonoff induction and AIXI. 

\subsection{Solomonoff induction}

Solomonoff induction is the theory of learning if you have unbounded computational power.

This was introduced in the original conference on AI in the 1950's. The form that we have today was perfected in the 1990's.

You can get everything by formalizing Occam's razor very hard. We want to believe the simplest theory consistent with all the data, i.e. the simplest computer program generating all the data so far.

We want a Bayesian prior probability that conforms to this.

\begin{df}
%Turing defined 
A \vocab{Turing machine} has three tapes.
\begin{enumerate}
\item
an output tape. Once it outputs a bit it can't rewrite it.
\item
working tape going infinitely in each direction. You can move in either direction, and rewrite as you want.
\item
input tape (program tape), can't go backwards.
\end{enumerate}

A \vocab{universal Turing machine} $U$ is such that for any other Turing machine $T$ in this form, 
there is a program, i.e., a finite string of bits which we can put as prefix to the input to the universal Turing machine $U$,\footnote{Make sure no program is a prefix of another program. The probability of a program is $2^{-l}$ where $l$ is the length of the encoding.} before the input to $T$, so that $U$ will give the same output.
% so that it with end at position $l$ and all the rest of the inputs will be treated as inputs to the machine $T$.
\end{df}

\begin{df}
The \vocab{universal semimeasure} is a distribution on infinite bitstrings. When we feed in random bits to the universal Turing machine, we get bits out. $M(p)$ is the probability of getting $p$ as prefix to the output.
%, get bits out that are somewhat random 
\end{df}
%stop asking for input

\begin{figure}
\ig{pics/solo1}{1}
\caption{Solomonoff induction}
\end{figure}

Halting issues make $M$ uncomputable, but also makes something interesting happen.

Given 0100111, what's the probability that the next bit is 0/1? They don't sum to 1 because there's a probability that the machine doesn't output anything further.

Solomonoff thought this was a problem. The Solomonoff normalization is as follows.
\begin{df}[Solomonoff measure]
Conditioned on a next bit being outputted, what is the probability that it is 0/1?
\end{df}
Note this is different from conditioning on strings that give infinite output.\footnote{For the Solomonoff measure there's the odd thing of ``jumping'' to a different model if a model stops giving output at a particular step.}

%It's the unique (?) machine 
The distribution has the following nice property: If $l$ is length of shortest program that gives everything so far, then the probability of the output is at least $2^{-l}$. This gives a lower bound on probability of specific observation.

%fuzzy between program and data

Doing science is about predicting sensory observations, trying to figure out what the input bits might be that emitted the particular output (scientific observations).

Bayes loss quantifies prediction error in sensory  observations. %Predicting some particular bit, output a probability $p'$. 
The loss is $-\lg (p')$ where $p'$ is the probability you assign to the event that happens (ex. bit that is output).
%deep dark Bayes hell

Any amount of Bayes loss is because the universal measure assigned probability $2^{-l}<1$ to the correct program.

When you do Bayesian update, you cut out all programs which don't give the correct next bit. You must be increasing the probability of the correct program by $\rc{p'}$. (Renormalize good programs to sum to 1.) I can only increase it so much before it's 1.

I.e., Bayes loss is at most $-\lg\prc{2^l}=l$.

This is nice but also dissatisfying: we assume computable environment. It would be nice to deal with stochastic worlds, randomness in lives. 
Maybe we'll never learn true equations of physics or use them to fully predict output.

Can Solomonoff induction predict good stochastic models of the world? Yes.

We can emulate stochastic Turing machines. Consider a Turing machine taking in input and using randomness to give output. Use the rest of the coin flips on input tape of the UTM as randomness. (You lose probability mass every time, but not more than you would lose representing a probabilistic hypothesis.)

%finite loss according to ML method you claim works well on the environment.
%I can write some program that does better.
%I can talk about some not deterministic computable...
%treat with my ML method of choice
%SI can learn to emulate if indeed best way to predict environment
%Can have infinite loss if infinite randomness

You get finite loss compared to whatever other machine learning algorithm that works in that environment. (Total loss can be infinite if there is infinite randomness.)

\subsection{AIXI}

%reinforcement learning agent
We define a universal (reinforcement learning) agent that uses the universal semimeasure or Solomonoff induction.

The universal Turing machine has a new tape that is the action tape. The agent gives the actions which the environment takes in.

Some number of bits forms a time step. $R$ is a function of these bits and outputs a reward.

\begin{figure}
\ig{pics/aixi1}{1}
\caption{AIXI}
\end{figure}

Have a finite horizon which you treat as the end of time. Make a tree of possible actions %from
back to the current time. Maximize reward.

We can do things to get rid of the horizon function: use discounted reward and take the limit forwards.
%how can we make this ``consistent'' - and not e.g. fail at some future point?
%or keep pushing forward
%cognitive biases
Exponential discounting is temporally consistent.
%only way to have time-consistent preferences
%ex. money
%won't modify inconsistently because of discounting function 

In what sense is this optimal?
You don't necessarily get to the optimal policy. 

It's doing best thing according to universal distribution. You can modify AIXI: do some exploration---ex. open a door to find out what happens. If you do exploration in the right way, you can converge to the right optimal policy. %The reason you converge
But if there may be traps in the environment---opening the door may lead to infinite hell. ``You're doing all the things you should be doing in hell.'' This is the sense that you can converge to the optimal policy.

%You need to explore to do well, go in the trap.
Do we want Bayes optimal, optimal thing that falls in traps,...? This is unclear.

%AIXI-js

%(It could have a fear that looking out the window has a terrible problem.)

%Explore to best of ability.
%best policy

%VOI - value of information.
%finite regret?

\section{Advice}

\begin{enumerate}
\item
Have high (self-)standards for your understanding.
\begin{itemize}
\item
Original, not novel: ``novel'' means first time anyone has the idea. ``Original'' means you can regenerate them, explain them, translate them.
\item
Counter-arguments: Run the counterargument train on yourself over and over again.
\end{itemize}
\item
Nimbleness  (Notice when you're going down same paths and do something else)
\begin{itemize}
\item
Boggle
\item
Pre/post-formal distinction: 
\begin{enumerate}
\item
Pre-formal means you don't do any proofs. Reason using intuition and algorithms but can't give rigorous statements of theorem. 
\item
Formal: Crystal concepts, concepts and definitions that slot into each other, clear semantics, write formal proofs. 
\item 
Post-formal: interact with underlying math reality but still reason in intuitive way, understand the stuff behind the symbols. 
\end{enumerate}•
\item ``Raw thoughts'' as epistemic status tag.
\end{itemize}
\item
Attend to the central mysteries, like naturalization.
Even as you're doing math, think in the back of your head of agents reasoning; match up math to something that might fit into an agent.
\item
Courage.
\end{enumerate}

%3,4,6 same

\section{Diagonalization}

We have a reflection property and fixed point property. In many cases there is a simple relationship.

\begin{df}
The \vocab{reflection} property: $g$ is a surjection of a set onto a set of functions whose domain is the same set.
\begin{align}
g:S\tra T^S
\end{align}
\end{df}
\begin{df}
The \vocab{fixed point} property: For 
\begin{align}
f:T\to T,
\end{align}
$x\in T$ with $f(x)=x$.
\end{df}
Roughly, if you have the reflection property than every function has a fixed point.

Sometimes we use this in the positive direction; sometimes we use the contrapositive.

Fixed point sounds more mathematical, and reflection sounds more like agent foundations.

$T$ is like states of robot. For each state, do something about that.

Think of $T^S$ as a set of functions from $S$ to $T$ (ex. computable functions, continuous functions, etc.). 

\subsection{Examples}

We have a general proof that generalizes the proof that $\neg \exists \N\tra P(\N) = 2^{\N}$. 

\begin{figure}
\ig{pics/diag1}{1}
\caption{Diagonalization for $\N\tra P(\N)=2^\N$
}
\end{figure}
To show that statement, consider
$$
h: x\mapsto f(g(x)(x))
$$
where $f(0)=1$ and $f(1)=0$. Suppose by way of contradiction there is $n\in \N$ such that $n\in \N$, $g(n)=h$. Then
\begin{align}
g(n)(n)= h(n) = f(g(n)(n)),
\end{align}
so $g(n)(n)$ is a fixed point, contradicting definition of $f$. 
The proof is even in the positive direction, it (constructively) gives a fixed point. (Picking the $n$ is less constructive.)

We can also look at topological examples. If we have a map $[0,1]\tra \bS^{[0,1]}$ where $\bS$ is the circle (just the boundary).

The sides are the interval. Each ``row''  is a line of directions. Can we have a map from the square to functions $\bS^{[0,1]}$ that lists out all possible maps? No, we can look at the diagonal. This is an ``abstract'' diagonal.

\begin{figure}
\ig{pics/diag2}{1}
\caption{Diagonalization for $[0,1]\tra \bS^{[0,1]}$}
\end{figure}

%if not continuous, not necessarily continuous

%that is like the solution to 3
%constructively give a fixed point.

Currying relates functions of multiple arguments and functions of 1 argument valued in function space. $\wt g(x)(y)$ is the curried version of $g(x,y)$. $g:\N \to P(\N)=2^\N$ corresponds to $\wt g:\N\times \N \to \{0,1\}$.

%\begin{thm}
More generally, let $f:T\to T$ and $g:S\to T^S$. For all $x$, consider the function $S\to T$ defined by
$$
s\mapsto f(g(s,s)).
$$
The fixed point comes from the element of the set that codes for the function. If there is no fixed point, then the map is not surjective.

Lawvere's fixed point theorem works for all cartesian closed categories (roughly, there is notion of composition of functions and products). %\footnote{You can write things that look like programs (cf. lambda calculus).} 
%all the problems are a corollary.
All the problems have the same kind of proof, and are corollaries of Lawvere's fixed point theorem.
%\end{thm}
%surjectivity assumption

%not entirely constructive  but mostly
%constructively have ``do something different'', apply $f$. 

Quines use the same idea: call function with code of function as argument. There's a version you can do in computability-land, fixed point is not halting. Modify the construction to not have that problem; this tells you how to make quines.

%write instructions

\subsection{Lambda calculus}
There are also positive uses of Lawvere's fixed point theorem.

Lambda calculus is a minimalistic programming language. %haskell without actual haskell functions or datatypes
Everything is a function, functions are notated literally as lambda terms. For example $(\la \,x.\,x)$ is the identity function, and
$$
(\la \,x.\,x)(\la \,x\,y.\,y\,x) = (\la \,x\,y.\,y\,x).
$$
We can add more datatypes to give better examples. Let's say we also have arithmetic.
Then
\begin{align}
(\la \,f\,x.\,f\,(f\,x))(\la\,y.\,y+3)(5) 
&= (\la\,y.\,y+3)((\la\,y.\,y+3)(5))\\
& =(\la\,y.\,y+3)(8)=11\\
(\la\,x.\la\,y.\la\,z.\,z\,x) \,7\, (\la \,a. \,a+a) \,(\la\,b.\, b- 5) &=
(\la\,y.\la\,z.\,z\,7) \, (\la \,a. \,a+a) \,(\la\,b.\, b- 5) \\
&=
(\la\,z.\,z\,7)  \,(\la\,b.\, b- 5)\\
&=(\la\,b.\, b- 5)\, 7=2\\
(\la\,x.\la\,y.\la\,z.\,y\,(z\,x)) \,(\la\,a.\,2\cdot a)\,(\la\,b.\,b+3)\,10 &=23. 
\end{align}

Given $f:\La \to \La$, I want a lambda term $x$ that evaluates to $f\,x$:
$$f\equiv f \,x .$$
This is asking for a fixed point property.

The functions $\La\to \La$ are exactly the lambda terms, 
$$\La = \La^\La$$
so $f\in \La$. (This can be made precise by domain theory.)

Here we have a bijection $\La \tra \La^\La$, lambda terms just are the right sort of function. $g$ is implicit.
Construct $x$ by the diagonal construction. 

The diagonal map is $y\mapsto y\,y$ which corresponds to $(\la \,y.\,y\,y)$, and the function is $y\mapsto f \, (\la \,y.\,y\,y)$ which corresponds to $\la \,y. f \, (\la \,y.\,y\,y)$. We get
\begin{align}
(\la \,y. f \, (\la \,y.\,y\,y))\,(\la \,y. f \, (\la \,y.\,y\,y))
&= f\, \pa{
(\la \,y. f \, (\la \,y.\,y\,y))\,(\la \,y. f \, (\la \,y.\,y\,y))
}
\end{align}
%Use this trick to have $x$ be applied to
This is the Y-combinator. You can use it to implement recursion.
\begin{align}
Y&=\la \,f .\,(\la \,y.\,f(y\,y))(\la \, y.\, f\, (y\, y)).
\end{align}
(Note: 2 lambda terms are the same if they can be reduced to the same thing. However, this equivalence is not computable.)

\subsection{Applications to logic}
%{L\"ob's theorem}

I'll discuss applications to logic and to Agent Foundations.

I'll do logic with as little logic as possible. Let $L$ be a language like the language of Peano arithmetic. Let $L_n$ be formulas with $n$ free variables.

These are sentences in $L_0$:
\begin{align}
2+2&=3\\
\forall x.\, \exists y.\, y&>x.
\end{align}
These are formulas in $L_1$:
\begin{align}
2+n&=3\\
\forall x.\, x+n&=x.
\end{align}
Here $x,y$ are quantified variables and $n$ is a free variable.

$2+2=3$ is a sentence and $\ce{2+2=3}$ is code for a sentence (a number).

%We have a common application to , the diagonal lemma. 
One application is the diagonal lemma.
\begin{lem}[Diagonal lemma]
For all $\ph\in L_1$, there is $\psi\in L_0$ such that 
$$
\vdash\ph(\ce{\psi}) \lra \ph.
$$
\end{lem}
``$\vdash$'' means provable, $\vdash 2+2=4$, $\not\vdash 2+2=3$.

%operate as if operate on strings, ex. can easily OR.

Tarski's undefinability theorem: is there a formula $T\in L_1$ that codes for a true sentence? Suppose there is
$$
T(\ce{\psi}) \lra \psi.
$$
By the Diagonal Lemma, since $\neg T(n)\in L_1$, we can find $\psi$ such that 
$$
\vdash \psi \lra \neg T(\ce{\psi}).
$$
%provability vs. truth/definability/expressibility
%we know there are some undefinable predicates because countably many definable
%come predicates undefinable

\subsubsection{L\"ob's theorem}

A second application of the diagonal lemma is L\"ob's Theorem. Given $\psi$, let $\square \ce{\psi}$ be the statement that $\psi$ is provable. This is definable. (There exists a number encoding a proof of it.)

Under which conditions is 
$$
\vdash \square \ce{\psi} \to \psi?
$$
%self-fulfilling proofs

The box is a formula in our language (that takes in number as input), and the turnstile is meta-mathematically saying that what's to the right can be proved. 
%if sound, anything provable is true

%G\''odel's incompleteness theorem says you can't have consistency and completeness at the same time.

This is true when $\psi$ is provable. Also for self-fulfilling proofs. (If agent can prove it turns left, it turns left.) %If whenever $\psi$ is provable. 
Are there other sentences for which this is provable? L\"ob's Theorem says no.

\begin{thm}[L\"ob's Theorem]
$$
\vdash \square \ce{\psi} \to \psi \implies \vdash \psi.
$$
\end{thm}

Suppose an agent is such that if the agent finds a proof that it goes left, it goes left. Then L\"ob's Theorem says it goes left.

L\"ob's sentence is constructed using the diagonal lemma.

In the context of an agent, this has weird consequences. We might want it to believe soundness about itself: believe things it's proven, i.e., assert anything it's proven is true. If it assigns probabilities, consider the sentences it assigns probability 1 to. It can self-referentially form the sentence
\begin{align}
\Pj(\phi)=1\to \phi.
\end{align}
This is soundness applied to belief rather than provability.

L\"ob's Theorem only uses simple facts about provability such as 
$$
\vdash \square \ce{\psi} \to \square \ce{\square \ce{\psi}}.
$$

In the belief case, you can recover many properties of provability. If agent assigns probability 1 to $\Pj(\phi)=1\to \phi$, it has to already believe (assign probability 1) to $\phi$. 
%limit way think about itself in way that it's surprising.

We can think about the agent as wanting to plan for its future self. It's a desirable property that the agent at time 0 will think it will form correct judgments at time 1. Then the agent at time 1 has to be doing proofs in a different theory than at time 0. If agent at time 1 is doing proofs in PA and so is the agent at time 0, then this property can't hold.
$$
\vdash_{ZFC} \square_{PA} \ce{\phi}\to \phi.
$$
Reduction in power is not a satisfying way of solving the problem.

There is a bounded variant of L\"ob's theorem (up to proofs of certain length).

%\forall\psi( \square \ce{\psi}\to \psi)\to \psi
%then every statement is provable, and not consistent.
%turnstil and box could refer to different systems. PA+soundness as turnstile.

%obfuscate trick from system itself
%unsound?

What if you're always in ZFC, trusting subcomponent of PA?

You can have partial self-modification as long as you can prove to trusted core that modification is OK. But you can't convince the trusted core to modify itself.

Coherence and reflection conditions on probability distribution.
\begin{df}
$\Pj$ is \vocab{coherent} if
\begin{enumerate}
\item
If $\vdash \perp \ph'$ then $\Pj(\ph)=1$. 
\item
If $\vdash \ph\to \psi$ then $\Pj(\ph) \le \Pj(\psi)$. 
\item
$\Pj(\ph\wedge \psi)+ \Pj(\ph\vee \psi) =\Pj(\ph) + \Ph(\psi)$.
%$\Pj(\ph)<a $ iff $\Pj(\Pj(\ph)<a)=1$.
\end{enumerate}
$\Pj$ has \vocab{reflection} if
$$
\Pj(\ph)<a \text{ iff } \Pj(\Pj(\ph)<a)=1.
$$
\end{df}
%LI far from coh?
$\Pj$ can't be coherent and reflective at the same time.

\subsubsection{L\"obian handshake}
There is 1 positive thing we can do with L\"ob's Theorem that lets us implement something we would want to implement: L\"obian handshakes to implement coordination in certain game-theoretic situations such as Prisoner's dilemma.

If the robot proves that it will turn left, it will turn left (assuming it is provable in the same system).

Consider 2 agents $A, B$, playing Prisoner's Dilemma, each cooperates if it can prove the other agent cooperates.
\begin{align}
A&=\text{coop} \lra \square \ce{B=\text{coop}}\\
B&=\text{coop} \lra  \square \ce{A=\text{coop}}.
\end{align}
We have $A=\text{coop}\wedge B=\text{coop} $ because
\begin{align}
\vdash& \square 
\ce{A=\text{coop} \wedge B=\text{coop}} \to 
A=\text{coop}\wedge B=\text{coop}\\
\vdash &A=\text{coop}\wedge B= \text{coop}.
\end{align}
They are looking for the cooperation handshake. Decide what handshake to look for, define what looking for means...
\begin{enumerate}
\item
If you replace coop with defect, then both defect.
\item
If A coops only if it proves B coops, and B defects only if it proves A defects. Neither can prove what the other does, so A defects and B cooperates.
\end{enumerate}•

%what if other defect
%not mutual coop
%neither can prove. 
%defect, coop




%\printbibliography
%\appendix


\end{document}

